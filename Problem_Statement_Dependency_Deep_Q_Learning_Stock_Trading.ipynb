{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem_Statement_Dependency_Deep Q Learning Stock Trading.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarthaSamuel/Reinforcement-Learning-/blob/main/Problem_Statement_Dependency_Deep_Q_Learning_Stock_Trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgy74k3DDaf_"
      },
      "source": [
        "#**Stock Trading Using Deep Q-Learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW3MPB3pDndR"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeOMTczqDqdZ"
      },
      "source": [
        "Prepare an agent by implementing Deep Q-Learning that can perform unsupervised trading in stock trade. The aim of this project is to train an agent that uses Q-learning and neural networks to predict the profit or loss by building a model and implementing it on a dataset that is available for evaluation.\n",
        "\n",
        "\n",
        "The stock trading index environment provides the agent with a set of actions:<br>\n",
        "* Buy<br>\n",
        "* Sell<br>\n",
        "* Sit\n",
        "\n",
        "This project has following sections:\n",
        "* Import libraries \n",
        "* Create a DQN agent\n",
        "* Preprocess the data\n",
        "* Train and build the model\n",
        "* Evaluate the model and agent\n",
        "<br><br>\n",
        "\n",
        "**Steps to perform**<br>\n",
        "\n",
        "In the section **create a DQN agent**, create a class called agent where:\n",
        "* Action size is defined as 3\n",
        "* Experience replay memory to deque is 1000\n",
        "* Empty list for stocks that has already been bought\n",
        "* The agent must possess the following hyperparameters:<br>\n",
        "  * gamma= 0.95<br>\n",
        "  * epsilon = 1.0<br>\n",
        "  * epsilon_final = 0.01<br>\n",
        "  * epsilon_decay = 0.995<br>\n",
        "\n",
        "\n",
        "    Note: It is advised to compare the results using different values in hyperparameters.\n",
        "\n",
        "* Neural network has 3 hidden layers\n",
        "* Action and experience replay are defined\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu4reAtsL5EZ"
      },
      "source": [
        "## **Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBgbvVTRDXpe"
      },
      "source": [
        "### **Import the libraries** "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "ujNj2USNZMi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt5QkvOCri3W"
      },
      "source": [
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import adam_v2\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behdrRbIDXpj"
      },
      "source": [
        "### **Create a DQN agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrJH6vRmNZQw"
      },
      "source": [
        "**Use the instruction below to prepare an agent**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7uHLPJWtNmm"
      },
      "source": [
        "# Action space include 3 actions: Buy, Sell, and Sit\n",
        "#Setting up the experience replay memory to deque with 1000 elements inside it\n",
        "#Empty list with inventory is created that contains the stocks that were already bought\n",
        "#Setting up gamma to 0.95, that helps to maximize the current reward over the long-term\n",
        "#Epsilon parameter determines whether to use a random action or to use the model for the action. \n",
        "#In the beginning random actions are encouraged, hence epsilon is set up to 1.0 when the model is not trained.\n",
        "#And over time the epsilon is reduced to 0.01 in order to decrease the random actions and use the trained model\n",
        "#We're then set the speed of decreasing epsililon in the epsilon_decay parameter\n",
        "\n",
        "#Defining our neural network:\n",
        "#Define the neural network function called _model and it just takes the keyword self\n",
        "#Define the model with Sequential()\n",
        "#Define states i.e. the previous n days and stock prices of the days\n",
        "#Defining 3 hidden layers in this network\n",
        "#Changing the activation function to relu because mean-squared error is used for the loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
        "        self.state_size = state_size # normalized previous days\n",
        "        self.action_size = 3 # sit, buy, sell\n",
        "        self.memory = deque(maxlen=1000)\n",
        "        self.inventory = []\n",
        "        self.model_name = model_name\n",
        "        self.is_eval = is_eval\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = load_model(model_name) if is_eval else self._model()\n",
        "    def _model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n",
        "        model.add(Dense(units=32, activation=\"relu\"))\n",
        "        model.add(Dense(units=8, activation=\"relu\"))\n",
        "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
        "        model.compile(loss=\"mse\", optimizer=adam_v2.Adam(lr=0.001))\n",
        "        return model\n",
        "    def act(self, state):\n",
        "        if not self.is_eval and random.random()<= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        options = self.model.predict(state)\n",
        "        return np.argmax(options[0])\n",
        "    def expReplay(self, batch_size):\n",
        "        mini_batch = []\n",
        "        l = len(self.memory)\n",
        "        for i in range(l - batch_size + 1, l):\n",
        "            mini_batch.append(self.memory[i])\n",
        "        for state, action, reward, next_state, done in mini_batch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nRItH8mDXpm"
      },
      "source": [
        "### **Preprocess the stock market data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3NGhJubtfet"
      },
      "source": [
        "import math\n",
        "\n",
        "# prints formatted price\n",
        "def formatPrice(n):\n",
        "\treturn (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
        "\n",
        "# returns the vector containing stock data from a fixed file\n",
        "def getStockDataVec(key):\n",
        "\tvec = []\n",
        "\tlines = open(\"\" + key + \".csv\", \"r\").read().splitlines()\n",
        "\n",
        "\tfor line in lines[1:]:\n",
        "\t\tvec.append(float(line.split(\",\")[4]))\n",
        "\n",
        "\treturn vec\n",
        "\n",
        "# returns the sigmoid\n",
        "def sigmoid(x):\n",
        "\treturn 1 / (1 + math.exp(-x))\n",
        "\n",
        "# returns an an n-day state representation ending at time t\n",
        "def getState(data, t, n):\n",
        "\td = t - n + 1\n",
        "\tblock = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "\tres = []\n",
        "\tfor i in range(n - 1):\n",
        "\t\tres.append(sigmoid(block[i + 1] - block[i]))\n",
        "\n",
        "\treturn np.array([res])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oHyPgfDXpp"
      },
      "source": [
        "### **Train and build the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCqJzkeJtp3n"
      },
      "source": [
        "'''import sys\n",
        "\n",
        "if len(sys.argv) != 4:\n",
        "\tprint (\"Usage: python train.py [stock] [window] [episodes]\")\n",
        "\texit()\n",
        "\n",
        "\n",
        "stock_name = input(\"Enter stock_name, window_size, Episode_count\")\n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Training_Dataset\n",
        "#window_size = 10\n",
        "#Episode_count = 100 or it can be 10 or 20 or 30 and so on.\n",
        "\n",
        "window_size = input()\n",
        "episode_count = input()\n",
        "stock_name = str(stock_name)\n",
        "window_size = int(window_size)\n",
        "episode_count = int(episode_count)\n",
        "\n",
        "agent = Agent(window_size)\n",
        "data = getStockDataVec(stock_name)\n",
        "l = len(data) - 1\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(episode_count + 1):\n",
        "\tprint (\"Episode \" + str(e) + \"/\" + str(episode_count))\n",
        "\tstate = getState(data, 0, window_size + 1)\n",
        "\n",
        "\ttotal_profit = 0\n",
        "\tagent.inventory = []\n",
        "\n",
        "\tfor t in range(l):\n",
        "\t\taction = agent.act(state)\n",
        "\n",
        "\t\t# sit\n",
        "\t\tnext_state = getState(data, t + 1, window_size + 1)\n",
        "\t\treward = 0\n",
        "\n",
        "\t\tif action == 1: # buy\n",
        "\t\t\tagent.inventory.append(data[t])\n",
        "\t\t\tprint (\"Buy: \" + formatPrice(data[t]))\n",
        "\n",
        "\t\telif action == 2 and len(agent.inventory) > 0: # sell\n",
        "\t\t\tbought_price = agent.inventory.pop(0)\n",
        "\t\t\treward = max(data[t] - bought_price, 0)\n",
        "\t\t\ttotal_profit += data[t] - bought_price\n",
        "\t\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
        "\n",
        "\t\tdone = True if t == l - 1 else False\n",
        "\t\tagent.memory.append((state, action, reward, next_state, done))\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\tif done:\n",
        "\t\t\tprint (\"--------------------------------\")\n",
        "\t\t\tprint (\"Total Profit: \" + formatPrice(total_profit))\n",
        "\t\t\t\n",
        "\n",
        "\t\tif len(agent.memory) > batch_size:\n",
        "\t\t\tagent.expReplay(batch_size)\n",
        "\n",
        "\t#if e % 10 == 0:\n",
        "\t\tagent.model.save(\"model_ep\" + str(e))'''\n",
        "\t\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "if len(sys.argv) != 4:\n",
        "\tprint (\"Usage: python train.py [stock] [window] [episodes]\")\n",
        "\texit()\n",
        "\n",
        "\n",
        "stock_name = input(\"Enter stock_name, window_size, Episode_count\")\n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Training_Dataset\n",
        "#window_size = 10\n",
        "#Episode_count = 100 or it can be 10 or 20 or 30 and so on.\n",
        "\n",
        "window_size = input()\n",
        "episode_count = input()\n",
        "stock_name = str(stock_name)\n",
        "window_size = int(window_size)\n",
        "episode_count = int(episode_count)\n",
        "\n",
        "agent = Agent(window_size)\n",
        "data = getStockDataVec(stock_name)\n",
        "l = 10\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(episode_count + 1):\n",
        "\tprint (\"Episode \" + str(e) + \"/\" + str(episode_count))\n",
        "\tstate = getState(data, 0, window_size + 1)\n",
        "\n",
        "\ttotal_profit = 0\n",
        "\tagent.inventory = []\n",
        "\n",
        "\tfor t in range(l):\n",
        "\t\taction = agent.act(state)\n",
        "\n",
        "\t\t# sit\n",
        "\t\tnext_state = getState(data, t + 1, window_size + 1)\n",
        "\t\treward = 0\n",
        "\n",
        "\t\tif action == 1: # buy\n",
        "\t\t\tagent.inventory.append(data[t])\n",
        "\t\t\tprint (\"Buy: \" + formatPrice(data[t]))\n",
        "\n",
        "\t\telif action == 2 and len(agent.inventory) > 0: # sell\n",
        "\t\t\tbought_price = agent.inventory.pop(0)\n",
        "\t\t\treward = max(data[t] - bought_price, 0)\n",
        "\t\t\ttotal_profit += data[t] - bought_price\n",
        "\t\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
        "\n",
        "\t\tdone = True if t == l - 1 else False\n",
        "\t\tagent.memory.append((state, action, reward, next_state, done))\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\tif done:\n",
        "\t\t\tprint (\"--------------------------------\")\n",
        "\t\t\tprint (\"Total Profit: \" + formatPrice(total_profit))\n",
        "\t\t\t\n",
        "\n",
        "\t\tif len(agent.memory) > batch_size:\n",
        "\t\t\tagent.expReplay(batch_size)\n",
        "\n",
        "\t#if e % 10 == 0:\n",
        "\t\tagent.model.save(\"model_ep\" + str(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcXCrJUSDXpr"
      },
      "source": [
        "### **Evaluate the model and agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZUVXe5t95k"
      },
      "source": [
        "'''import sys\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "if len(sys.argv) != 3:\n",
        "\tprint (\"Usage: python evaluate.py [stock] [model]\")\n",
        "\texit()\n",
        "\n",
        "\n",
        "stock_name = input(\"Enter Stock_name, Model_name\")\n",
        "model_name = input()\n",
        "#Note: \n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Evaluation_Dataset\n",
        "#Model_name = respective model name\n",
        "\n",
        "model = load_model(\"\" + model_name)\n",
        "window_size = model.layers[0].input.shape.as_list()[1]\n",
        "\n",
        "agent = Agent(window_size, True, model_name)\n",
        "data = getStockDataVec(stock_name)\n",
        "l = len(data) - 1\n",
        "batch_size = 32\n",
        "\n",
        "state = getState(data, 0, window_size + 1)\n",
        "total_profit = 0\n",
        "agent.inventory = []\n",
        "\n",
        "for t in range(l):\n",
        "\taction = agent.act(state)\n",
        "\n",
        "\t# sit\n",
        "\tnext_state = getState(data, t + 1, window_size + 1)\n",
        "\treward = 0\n",
        "\n",
        "\tif action == 1: # buy\n",
        "\t\tagent.inventory.append(data[t])\n",
        "\t\tprint (\"Buy: \" + formatPrice(data[t]))\n",
        "\n",
        "\telif action == 2 and len(agent.inventory) > 0: # sell\n",
        "\t\tbought_price = agent.inventory.pop(0)\n",
        "\t\treward = max(data[t] - bought_price, 0)\n",
        "\t\ttotal_profit += data[t] - bought_price\n",
        "\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
        "\n",
        "\tdone = True if t == l - 1 else False\n",
        "\tagent.memory.append((state, action, reward, next_state, done))\n",
        "\tstate = next_state\n",
        "\n",
        "\tif done:\n",
        "\t\tprint (\"--------------------------------\")\n",
        "\t\tprint (stock_name + \" Total Profit: \" + formatPrice(total_profit))'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "if len(sys.argv) != 3:\n",
        "\tprint (\"Usage: python evaluate.py [stock] [model]\")\n",
        "\texit()\n",
        "\n",
        "\n",
        "stock_name = input(\"Enter Stock_name, Model_name\")\n",
        "model_name = input()\n",
        "#Note: \n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Evaluation_Dataset\n",
        "#Model_name = respective model name\n",
        "\n",
        "model = load_model(\"\" + model_name)\n",
        "window_size = model.layers[0].input.shape.as_list()[1]\n",
        "\n",
        "agent = Agent(window_size, True, model_name)\n",
        "data = getStockDataVec(stock_name)\n",
        "l = 10\n",
        "batch_size = 32\n",
        "\n",
        "state = getState(data, 0, window_size + 1)\n",
        "total_profit = 0\n",
        "agent.inventory = []\n",
        "\n",
        "for t in range(l):\n",
        "\taction = agent.act(state)\n",
        "\n",
        "\t# sit\n",
        "\tnext_state = getState(data, t + 1, window_size + 1)\n",
        "\treward = 0\n",
        "\n",
        "\tif action == 1: # buy\n",
        "\t\tagent.inventory.append(data[t])\n",
        "\t\tprint (\"Buy: \" + formatPrice(data[t]))\n",
        "\n",
        "\telif action == 2 and len(agent.inventory) > 0: # sell\n",
        "\t\tbought_price = agent.inventory.pop(0)\n",
        "\t\treward = max(data[t] - bought_price, 0)\n",
        "\t\ttotal_profit += data[t] - bought_price\n",
        "\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
        "\n",
        "\tdone = True if t == l - 1 else False\n",
        "\tagent.memory.append((state, action, reward, next_state, done))\n",
        "\tstate = next_state\n",
        "\n",
        "\tif done:\n",
        "\t\tprint (\"--------------------------------\")\n",
        "\t\tprint (stock_name + \" Total Profit: \" + formatPrice(total_profit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIolgCRWSM-9"
      },
      "source": [
        "**Note: Run the training section for considerable episodes so that while evaluating the model it can generate significant profit.** \n"
      ]
    }
  ]
}